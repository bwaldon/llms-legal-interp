{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682e748-c6a8-42c3-86c1-88bb94711d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 01:57:16 __init__.py:207] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd080e969f54a979ed06292d91c11a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 01:57:32 config.py:549] This model supports multiple tasks: {'generate', 'reward', 'score', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 03-19 01:57:32 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5056a964f16b404699769fcc8ee90cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a899f209d74146d0b6109924f6c0ee27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c7ec8aaee64ec19a4950c4d719a48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 01:57:36 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-19 01:57:37 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "INFO 03-19 01:57:38 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e587ea3729047b984145c234737f7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 01:57:56 weight_utils.py:270] Time spent downloading weights for deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B: 17.906740 seconds\n",
      "INFO 03-19 01:57:56 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e7c37d05d84d29ad63623457a31c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 01:58:20 model_runner.py:1115] Loading model weights took 3.3460 GB\n",
      "INFO 03-19 01:58:22 worker.py:267] Memory profiling takes 2.16 seconds\n",
      "INFO 03-19 01:58:22 worker.py:267] the current vLLM instance can use total_gpu_memory (21.96GiB) x gpu_memory_utilization (0.90) = 19.77GiB\n",
      "INFO 03-19 01:58:22 worker.py:267] model weights take 3.35GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 14.98GiB.\n",
      "INFO 03-19 01:58:22 executor_base.py:111] # cuda blocks: 35068, # CPU blocks: 9362\n",
      "INFO 03-19 01:58:22 executor_base.py:116] Maximum concurrency for 512 tokens per request: 1095.88x\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "# llm = LLM(model=\"facebook/opt-125m\")\n",
    "llm = LLM(model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", max_model_len=512, tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "168880e4-8ed2-4ae5-ad20-dee963a9285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"vague_contracts/data/1_falseconsensus/demo-merged.csv\") as f:\n",
    "    r = csv.reader(f, delimiter=\",\", quotechar='\"')\n",
    "    data = [row for row in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c8f6ee9-3b41-477a-97ef-4967432896ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Vance has a DJ business where he goes around town to work venues out of his van. His van has special audio equipment as it is Vance's only vehicle for his business. The van is relatively old and its warranty has expired for a few years. Vance receives a call one day for a DJ opportunity in a nearby town. On the way, he suffers a breakdown, likely from spark plugs that have worn down. When he calls his insurance to file a claim, they deny his request and inform him that mechanical breakdowns are not covered.\",\n",
       " 'Vance has an insurance policy against loss or damage to his goods carrying vehicle defined as \"loss or damage to the vehicle, audio visual communication and guidance systems, replacement for key theft, and recovery and redelivery.\" ',\n",
       " 'unambiguous_uncovered')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_relevant_fields(row):\n",
    "    return row[3], row[4], row[11]\n",
    "\n",
    "get_relevant_fields(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c8ced0-67e7-4014-b3fd-f45ee9f301f1",
   "metadata": {},
   "outputs": [],
   "source": "_p = f\"Can you tell me whether the filed claim should be denied or approved? Vance has an insurance policy against loss or damage to his goods carrying vehicle defined as loss or damage to the vehicle, audio visual communication and guidance systems, replacement for key theft, and recovery and redelivery. Vance has a DJ business where he goes around town to work venues out of his van. His van has special audio equipment as it is Vance's only vehicle for his business. The van is relatively old and its warranty has expired for a few years. Vance receives a call one day for a DJ opportunity in a nearby town. On the way, he suffers a breakdown, likely from spark plugs that have worn down. When he calls his insurance to file a claim, they deny his request and inform him that mechanical breakdowns are not covered.\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18583b-9c44-4e18-aec6-5982bbac5a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab55fcc-eb2d-4942-a530-5ab2c1017a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acffb186-bf75-48af-9fc9-a2b4a5490271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307acab-f2c1-4046-94a6-461e91a018c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
