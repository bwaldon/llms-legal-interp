{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e478166d-0ba7-483e-81a1-dc55a5c8fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import output_processing as op\n",
    "from prompts import candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77135457-4530-488d-9a0c-717147b385ce",
   "metadata": {},
   "source": [
    "# GPT-4 output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5f2f0-8c9e-4448-a7e8-9e686787aa1a",
   "metadata": {},
   "source": [
    "def read_and_organize_model_results(model_name):\n",
    "    model_results = pd.read_csv(f\"runs/runs-42_07_16/{model_name}-results.csv\")\n",
    "    print(np.sum(model_results == 0.0))\n",
    "    probs_columns = [candidate + \"_probs\" for candidate in candidates]\n",
    "    # option 1\n",
    "    # model_results.replace([0.0], -np.finfo(float).max, inplace=True)\n",
    "    # option 2, set others to prob 0\n",
    "    model_results = op.organize_distribution(model_results)\n",
    "    def clean_other_probs_sure_answer():\n",
    "        mask = model_results == 0.0\n",
    "        # We only want the non-columns\n",
    "        model_results.loc[mask]\n",
    "        return model_results\n",
    "    clean_other_probs_sure_answer()\n",
    "    # for candidate in candidates: \n",
    "    #     probs_columns = [candidate + \"_probs\" for candidate in candidates]\n",
    "\n",
    "    # print(\"Option 1:\", np.sum((model_results[\"Yes_probs\"] + model_results[\"No_probs\"]) > 1))\n",
    "    model_results[\"model_name\"] = model_name.split(\"/\")[-1]\n",
    "    model_results.loc[model_results[\"Covered\"] == True, \"Judgment\"] = \"Covered\"\n",
    "    model_results.loc[model_results[\"Covered\"] == True, \"Judgment_prob\"] = model_results[\"Covered_prob\"]\n",
    "    model_results.loc[model_results[\"NotCovered\"] == True, \"Judgment\"] = \"NotCovered\"\n",
    "    model_results.loc[model_results[\"NotCovered\"] == True, \"Judgment_prob\"] = model_results[\"NotCovered_prob\"]\n",
    "    return model_results\n",
    "\n",
    "model_results = read_and_organize_model_results(\"openai-gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a11c0e0b-fb66-4b4f-a0a5-f25ad087ea04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>prompt_type</th>\n",
       "      <th>prompt</th>\n",
       "      <th>version</th>\n",
       "      <th>output</th>\n",
       "      <th>output_text</th>\n",
       "      <th>YES_probs</th>\n",
       "      <th>Yes_probs</th>\n",
       "      <th>yes_probs</th>\n",
       "      <th>NO_probs</th>\n",
       "      <th>...</th>\n",
       "      <th>Aff_prob</th>\n",
       "      <th>UnAff_prob</th>\n",
       "      <th>Covered_prob</th>\n",
       "      <th>NotCovered_prob</th>\n",
       "      <th>Covered</th>\n",
       "      <th>NotCovered</th>\n",
       "      <th>entropy</th>\n",
       "      <th>model_name</th>\n",
       "      <th>Judgment</th>\n",
       "      <th>Judgment_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>Hot Work II</td>\n",
       "      <td>disagreement</td>\n",
       "      <td>Martha has an insurance policy that covers acc...</td>\n",
       "      <td>controversial</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.303124</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696874</td>\n",
       "      <td>0.303124</td>\n",
       "      <td>0.696874</td>\n",
       "      <td>0.303124</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.613515</td>\n",
       "      <td>openai-gpt-4</td>\n",
       "      <td>Covered</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           title   prompt_type  \\\n",
       "734  Hot Work II  disagreement   \n",
       "\n",
       "                                                prompt        version output  \\\n",
       "734  Martha has an insurance policy that covers acc...  controversial     No   \n",
       "\n",
       "    output_text  YES_probs  Yes_probs  yes_probs  NO_probs  ...  Aff_prob  \\\n",
       "734          No    0.00001   0.303124   0.000026  0.000007  ...  0.696874   \n",
       "\n",
       "     UnAff_prob  Covered_prob  NotCovered_prob  Covered  NotCovered   entropy  \\\n",
       "734    0.303124      0.696874         0.303124     True       False  0.613515   \n",
       "\n",
       "       model_name  Judgment  Judgment_prob  \n",
       "734  openai-gpt-4   Covered       0.696874  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results.sample()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdf4dce6-a906-459b-92af-7abc64fb9f2d",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35559a76-948a-46dd-bb3a-5e0c01d2a9d1",
   "metadata": {},
   "source": [
    "## OpenAI responses have > 1 total probability\n",
    "https://cookbook.openai.com/examples/using_logprobs\n",
    "\n",
    "OpenAI APIs provide logprob value 0.0 to indicate a token probability of 1\n",
    "However, it assigns non-zero probability to tokens leading to a probability estimation > 1 over the vocabulary\n",
    "\n",
    "There are two options to handle this\n",
    "1. Assign the smallest non-zero negative logprob (`-MIN_FLOAT`)\n",
    "2. Set others to 0\n",
    "3. Normalize the probabilities over the top_20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a89f0-5924-43ee-885b-56b805918ce5",
   "metadata": {},
   "source": [
    "## Does option 1, still lead to total probs > 1 : Yes for 200 cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b9dc1-8eac-4aee-91c5-d4b4712b8218",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307553b-a7ea-4dac-bce7-a2c2aaf0018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7902c3fb-da26-41fc-94b3-a9c7c392bb26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prej-prompt",
   "language": "python",
   "name": "prej-prompt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
